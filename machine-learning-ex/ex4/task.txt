Session 1:
1. load data:
    load('ex4data1.mat');
    m = size(X, 1);
    
    % Randomly select 100 data points to display
    sel = randperm(size(X, 1));
    sel = sel(1:100);
    displayData(X(sel, :));

2. load parameters:
    % Load the weights into variables Theta1 and Theta2
    load('ex4weights.mat');

3. nnCostFunction.m:
    X = [ones(m, 1) X]
    
    % Forward propagation
    a_1 = X    % 5000 * 401
    z_2 = Theta1 * a_1'     % (25 * 401) * (401 * 5000) = 25 * 5000
    a_2 = [ones(m,1) sigmoid(z_2)']       %26 * 5000
    z_3 = Theta2 * a_2'     % (10 * 26) * (26 * 5000) = 10 * 5000
    h_theta = sigmoid(z_3)      % 10 * 5000
    
    y_new = zeros(num_labels, m)        % 10 * 5000
    
    for i=1:m
      y_new(y(i),i)=1;
    end
    
    J = (1/m) * sum(sum((-y_new) .* log(h_theta) - (1 - y_new) .* log(1 - h_theta)))

4. run:
    input_layer_size  = 400;  % 20x20 Input Images of Digits
    hidden_layer_size = 25;   % 25 hidden units
    num_labels = 10;          % 10 labels, from 1 to 10 (note that we have mapped "0" to label 10)
    
    % Unroll parameters 
    nn_params = [Theta1(:) ; Theta2(:)];
    
    % Weight regularization parameter (we set this to 0 here).
    lambda = 0;
    
    J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda);
    
    fprintf('Cost at parameters (loaded from ex4weights): %f', J);

    (submit)

5. nnCostFunction.m (For regularized cost function):
    % For Regulized Cost Function
    t_1 = Theta1(:, 2: size(Theta1,2))
    t_2 = Theta2(:, 2: size(Theta2,2))
    
    reg_val = (lambda /(2*m))* (sum(sum(t_1 .^2)) + sum(sum(t_2 .^ 2)))
    
    J = J + reg_val

6. run:
    % Weight regularization parameter (we set this to 1 here).
    lambda = 1;
    
    J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda);
    fprintf('Cost at parameters (loaded from ex4weights): %f', J);

    (submit)

7. sigmoidGradient.m:
    g = sigmoid(z) .* (1 - sigmoid(z))

8. run:
    % Call your sigmoidGradient function
    sigmoidGradient(0)

    (submit)

9. randInitializeWeights.m:
    % Randomly initialize the weights to small values
    epsilon_init = 0.12;
    W = rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init;

10. run: 
    initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);
    initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);
    
    % Unroll parameters
    initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];
11. nnCostFunction.m:
    
% For background propagation
for i=1:m
    % 1
    a_1 = X(i, :)   %  1 * 401
    z_2= Theta1 * a_1'     % (25 * 401) * (401 * 1) = 25 * 1
    a_2 = sigmoid(z_2) % 26 * 1
   % size(a_2)
    a_2 = [1; a_2]
    z_3= Theta2 * a_2      % (10 * 26) * (26 * 1) = 10 * 1
    a_3 = sigmoid(z_3)      % 10 * 1
    
    % 2
    d_3 = a_3 - y_new(:, i)     % 10 *1
   % size(z_2)
    z_2 = [1; z_2]  % 26 * 1
    d_2 = (Theta2' * d_3) .* sigmoidGradient(z_2)    % (26 * 10) * (10 *1) = (26 * 1) + (26 *1)= 26 * 1
    
    % 3
    d_2 = d_2(2 : length(d_2))        % 25 *1
    Theta2_grad = Theta2_grad + d_3 * a_2'    %(10 *1) * (1 * 26) = 10 *26 
    Theta1_grad = Theta1_grad + d_2 * a_1    % (25 * 1) *( 1 * 401) = 25 * 401
      
    
 end

% Step 5
Theta2_grad = (1/m) * Theta2_grad; % (10*26)
Theta1_grad = (1/m) * Theta1_grad; % (25*401)

12. run : checkNNGradients
13. run: computeNumericGradient //optional
14. nnCostFunction.m:
    % Regularizaton
    %Theta1_grad(:, 1) = Theat1_grad(:, 1)/m    % j=0
    Theta1_grad(:, 2:end) = Theta1_grad(:, 2 : end) + lambda * Theta1(:, 2: end)/m     % j>=1 
    %Theta2_grad(:, 1) = Theat2_grad(:, 1)/m    % j=0
    Theta2_grad (:, 2: end)= Theta2_grad(:, 2:end) + lambda* Theta2(:, 2: end)/m     % j>=1
15. run:
    %  Check gradients by running checkNNGradients
    lambda = 3;
    checkNNGradients(lambda);
    % Also output the costFunction debugging value 
    % This value should be about 0.576051
    debug_J  = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda);
    fprintf('Cost at (fixed) debugging parameters (w/ lambda = 3): %f', debug_J);

16. run: checkNNGradients
 submit & finsh the task.
17. accuracy:
    options = optimset('MaxIter', 50);
    lambda = 1;
    
    % Create "short hand" for the cost function to be minimized
    costFunction = @(p) nnCostFunction(p, input_layer_size, hidden_layer_size, num_labels, X, y, lambda);
    
    % Now, costFunction is a function that takes in only one argument (the
    % neural network parameters)
    [nn_params, ~] = fmincg(costFunction, initial_nn_params, options);
    % Obtain Theta1 and Theta2 back from nn_params
    Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), hidden_layer_size, (input_layer_size + 1));
    Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), num_labels, (hidden_layer_size + 1));
     
    pred = predict(Theta1, Theta2, X);
    fprintf('\nTraining Set Accuracy: %f\n', mean(double(pred == y)) * 100);
18. 