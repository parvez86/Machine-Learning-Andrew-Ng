Session 1.
1. Load the data:
    data = load('ex2data1.txt');
    X = data(:, [1, 2]); y = data(:, 3);
2.  plotData.m:
     pos = find(y==1); neg = find(y == 0);
    % Plot Examples
    plot(X(pos, 1), X(pos, 2), 'k+','LineWidth', 2, 'MarkerSize', 7);
    plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y','MarkerSize', 7);
    % Labels and Legend
    xlabel('Exam 1 score')
    ylabel('Exam 2 score')
    
    % Specified in plot order
    legend('Admitted', 'Not admitted')

3. run: plotData(X, y)

optional exercise s done.
4. sigmoid.m:
    f = 1/(1+ e^-x) use this formula
5. run: submit & get 5 marks
6. sigmoid.m:
    h_theta = sigmoid(X*theta);
    J = (1 / m) * ((-y' * log(h_theta)) - (1 - y)' * log(1 - h_theta));
    
    grad = (1 / m) * (h_theta - y)' * X;
7. run:
    %  Setup the data matrix appropriately
    [m, n] = size(X);
    
    % Add intercept term to X
    X = [ones(m, 1) X];
    
    % Initialize the fitting parameters
    initial_theta = zeros(n + 1, 1);
    
    % Compute and display the initial cost and gradient
    [cost, grad] = costFunction(initial_theta, X, y);
    fprintf('Cost at initial theta (zeros): %f\n', cost);
    disp('Gradient at initial theta (zeros):'); disp(grad);

8. run:
    options = optimoptions(@fminunc,'Algorithm','Quasi-Newton','GradObj', 'on', 'MaxIter', 400);

    %  Run fminunc to obtain the optimal theta
    %  This function will return theta and the cost 
    [theta, cost] = fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);
    
    % Print theta
    fprintf('Cost at theta found by fminunc: %f\n', cost);
    disp('theta:');disp(theta);
    
    % Plot Boundary
    plotDecisionBoundary(theta, X, y);
    % Add some labels 
    hold on;
    % Labels and Legend
    xlabel('Exam 1 score')
    ylabel('Exam 2 score')
    % Specified in plot order
    legend('Admitted', 'Not admitted')
    hold off;

9. predict.m:
    p = sigmoid( X * theta) >= 0.5

10. run:
    %  Predict probability for a student with score 45 on exam 1  and score 85 on exam 2 
    prob = sigmoid([1 45 85] * theta);
    fprintf('For a student with scores 45 and 85, we predict an admission probability of %f\n\n', prob);
    % Compute accuracy on our training set
    p = predict(theta, X);
    fprintf('Train Accuracy: %f\n', mean(double(p == y)) * 100);
sumit & done 70% work.

Session 2:
1. run:
    %  The first two columns contains the X values and the third column
    %  contains the label (y).
    data = load('ex2data2.txt');
    X = data(:, [1, 2]); y = data(:, 3);
    
    plotData(X, y);
    % Put some labels 
    hold on;
    % Labels and Legend
    xlabel('Microchip Test 1')
    ylabel('Microchip Test 2')
    % Specified in plot order
    legend('y = 1', 'y = 0')
    hold off;

2. run:
    % Add Polynomial Features
    % Note that mapFeature also adds a column of ones for us, so the intercept term is handled
    X = mapFeature(X(:,1), X(:,2));

3. costFunctionreg.m:
    h_theta = sigmoid(X*theta);
    J = (1/m) * (-y' * log(h_theta) - (1-y)' * log(1-h_theta)) + (lambda/(2*m)) * (theta(2:length(theta)))' * theta(2:length(theta));
    
    thetaZero = theta;
    thetaZero(1) = 0;
    
    grad = ((1 / m) * (h_theta - y)' * X) + lambda / m * thetaZero';

4. run: 
    % Initialize fitting parameters
    initial_theta = zeros(size(X, 2), 1);
    
    % Set regularization parameter lambda to 1
    lambda = 1;
    
    % Compute and display initial cost and gradient for regularized logistic regression
    [cost, grad] = costFunctionReg(initial_theta, X, y, lambda);
    fprintf('Cost at initial theta (zeros): %f\n', cost);

 submit & finish the primary work.

5. run:
    % Initialize fitting parameters
    initial_theta = zeros(size(X, 2), 1);
    
    lambda = 0;
    % Set Options
    options = optimoptions(@fminunc,'Algorithm','Quasi-Newton','GradObj', 'on', 'MaxIter', 1000);
    
    % Optimize
    [theta, J, exit_flag] = fminunc(@(t)(costFunctionReg(t, X, y, lambda)), initial_theta, options);
    % Plot Boundary
    plotDecisionBoundary(theta, X, y);
    hold on;
    title(sprintf('lambda = %g', lambda))
    
    % Labels and Legend
    xlabel('Microchip Test 1')
    ylabel('Microchip Test 2')
    
    legend('y = 1', 'y = 0', 'Decision boundary')
    hold off;
    
    % Compute accuracy on our training set
    p = predict(theta, X);
    
    fprintf('Train Accuracy: %f\n', mean(double(p == y)) * 100);