Session 1:
1. load & visualize data:
    % Load from ex5data1: 
    % You will have X, y, Xval, yval, Xtest, ytest in your environment
    load ('ex5data1.mat');
    % m = Number of examples
    m = size(X, 1);
    
    % Plot training data
    figure;
    plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
    xlabel('Change in water level (x)');
    ylabel('Water flowing out of the dam (y)');

2. linearRegCostFunction.m:
    h_theta = X * theta     %(12 * 2)*(2* 1) = 12 * 1
    J = sum(sum((h_theta - y) .^ 2))/(2*m)
3. run:
    theta = [1 ; 1];
    J = linearRegCostFunction([ones(m, 1) X], y, theta, 1);
    fprintf('Cost at theta = [1 ; 1]: %f', J);

(submit)

4. linearRegCostFunction.m:
    theta(1) = 0
    reg_val = lambda * (sum(sum(theta .^2)))/(2*m)
    
    J = J + reg_val
    
    grad  = (1/m)* (X' * (h_theta - y)) + (lambda/m)* theta
5. run:
    [J, grad] = linearRegCostFunction([ones(m, 1) X], y, theta, 1);
fprintf('Gradient at theta = [1 ; 1]:  [%f; %f] \n',grad(1), grad(2));

(submit)

6. trainLinearReg.m:
    see the optimization.
7. run:
    %  Train linear regression with lambda = 0
    lambda = 0;
    [theta] = trainLinearReg([ones(m, 1) X], y, lambda);
8. run (visualize the optimizaton):
    %  Plot fit over the data
    figure;
    plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
    xlabel('Change in water level (x)');
    ylabel('Water flowing out of the dam (y)');
    hold on;
    plot(X, [ones(m, 1) X]*theta, '--', 'LineWidth', 2)
    hold off;

9. learningCurve.m:
    for i = 1:m
    x_sub = X(1:i, :)
    y_sub = y(1:i)
    theta = trainLinearReg(x_sub, y_sub, lambda);
    error_train(i) = linearRegCostFunction(x_sub, y_sub, theta, 0);
    error_val(i)= linearRegCostFunction(Xval, yval, theta, 0);
    end

10. run:
    lambda = 0;
    [error_train, error_val] = learningCurve([ones(m, 1) X], y, [ones(size(Xval, 1), 1) Xval], yval, lambda);
    
    plot(1:m, error_train, 1:m, error_val);
    title('Learning curve for linear regression')
    legend('Train', 'Cross Validation')
    xlabel('Number of training examples')
    ylabel('Error')
    axis([0 13 0 150])
    
    fprintf('# Training Examples\tTrain Error\tCross Validation Error\n');
    for i = 1:m
        fprintf('  \t%d\t\t%f\t%f\n', i, error_train(i), error_val(i));
    end

(submit)

11. polyFeatures.m:
    m = size(X, 1)
    for i=1:m
        poly_i = zeros(p, 1)
        for j=1:p
           poly_i(j) = X(i) .^ j
        end
        X_poly(i,:)= poly_i
    end
12. run:
    p = 8;

    % Map X onto Polynomial Features and Normalize
    X_poly = polyFeatures(X, p);
    [X_poly, mu, sigma] = featureNormalize(X_poly);  % Normalize
    X_poly = [ones(m, 1), X_poly];                   % Add Ones
    
    % % Map X_poly_test and normalize (using mu and sigma)
    X_poly_test = polyFeatures(Xtest, p);
    X_poly_test = X_poly_test-mu; % uses implicit expansion instead of bsxfun
    X_poly_test = X_poly_test./sigma; % uses implicit expansion instead of bsxfun
    X_poly_test = [ones(size(X_poly_test, 1), 1), X_poly_test];         % Add Ones
    
    % Map X_poly_val and normalize (using mu and sigma)
    X_poly_val = polyFeatures(Xval, p);
    X_poly_val = X_poly_val-mu; % uses implicit expansion instead of bsxfun
    X_poly_val = X_poly_val./sigma; % uses implicit expansion instead of bsxfun
    X_poly_val = [ones(size(X_poly_val, 1), 1), X_poly_val];           % Add Ones
    
    fprintf('Normalized Training Example 1:\n');
    fprintf('  %f  \n', X_poly(1, :));
    
    lambda = 0;
    [theta] = trainLinearReg(X_poly, y, lambda);

13. run:
    % Plot training data and fit
    plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
    plotFit(min(X), max(X), mu, sigma, theta, p);
    xlabel('Change in water level (x)');
    ylabel('Water flowing out of the dam (y)');
    title (sprintf('Polynomial Regression Fit (lambda = %f)', lambda));
    [error_train, error_val] = learningCurve(X_poly, y, X_poly_val, yval, lambda);
    plot(1:m, error_train, 1:m, error_val);
    title(sprintf('Polynomial Regression Learning Curve (lambda = %f)', lambda));
    xlabel('Number of training examples')
    ylabel('Error')
    axis([0 13 0 100])
    legend('Train', 'Cross Validation')

(submit)

14. run:
    % Choose the value of lambda
    lambda = 1;
    [theta] = trainLinearReg(X_poly, y, lambda);
    
    % Plot training data and fit
    plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
    plotFit(min(X), max(X), mu, sigma, theta, p);
    xlabel('Change in water level (x)');
    ylabel('Water flowing out of the dam (y)');
    title (sprintf('Polynomial Regression Fit (lambda = %f)', lambda));
    [error_train, error_val] = learningCurve(X_poly, y, X_poly_val, yval, lambda);
    plot(1:m, error_train, 1:m, error_val);
    title(sprintf('Polynomial Regression Learning Curve (lambda = %f)', lambda));
    xlabel('Number of training examples')
    ylabel('Error')
    axis([0 13 0 100])
    legend('Train', 'Cross Validation')

15. validationCurve.m:
    m = length(lambda_vec)
    
    for i=1:m
        lambda = lambda_vec(i)
        theta = trainLinearReg(x_sub, y_sub, lambda);
        error_train(i) = linearRegCostFunction(x_sub, y_sub, theta, lambda);
        error_val(i)= linearRegCostFunction(Xval, yval, theta, lambda);
    end
16. run:
    [lambda_vec, error_train, error_val] = validationCurve(X_poly, y, X_poly_val, yval);
    plot(lambda_vec, error_train, lambda_vec, error_val);
    legend('Train', 'Cross Validation');
    xlabel('lambda');
    ylabel('Error');
    for i = 1:length(lambda_vec)
        if i == 1
            fprintf('lambda\t\tTrain Error\tValidation Error\n');
        end
        fprintf('%f\t%f\t%f\n',lambda_vec(i), error_train(i), error_val(i));
    end